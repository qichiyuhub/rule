import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler
from go_parser import GoTransformParser
from typing import Tuple, List, Optional

# ==============================================================================
# 1. é…ç½®ä¸­å¿ƒ (CONFIGURATIONS)
# ==============================================================================

# æ–‡ä»¶è·¯å¾„é…ç½®
DATA_FILE = 'smart_weight_data.csv' 
GO_FILE = 'transform.go'
MODEL_FILE = 'Model.bin'

# ç‰¹å¾å˜æ¢é…ç½®
STD_SCALER_FEATURES = [
    'connect_time', 'latency', 'upload_mb', 'download_mb', 'duration_minutes', 
    'last_used_seconds', 'traffic_density'
]
ROBUST_SCALER_FEATURES = ['success', 'failure']

# LightGBM æ¨¡å‹å‚æ•°
LGBM_PARAMS = {
    'objective': 'regression', 'metric': 'rmse', 'n_estimators': 1000,
    'learning_rate': 0.03, 'random_state': 42, 'n_jobs': -1, 'device': 'gpu'
}
EARLY_STOPPING_ROUNDS = 100


# ==============================================================================
# 2. åŠŸèƒ½å‡½æ•° (FUNCTIONS)
# ==============================================================================

def load_and_clean_data(file_path: str) -> Optional[pd.DataFrame]:
    """ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®å¹¶è¿›è¡Œæ¸…æ´—ã€‚å¦‚æœå¤±è´¥åˆ™è¿”å›Noneã€‚"""
    print(f"--> æ­£åœ¨åŠ è½½æ•°æ®: {file_path}")
    try:
        data = pd.read_csv(file_path)
        print(f"    åŸå§‹æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(data)} æ¡ã€‚")
    except FileNotFoundError:
        print(f"    é”™è¯¯: æ•°æ®æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°!")
        return None

    data.dropna(subset=['weight'], inplace=True)
    data = data[data['weight'] > 0].copy()
    print(f"    æ¸…æ´—åå‰©ä½™ {len(data)} æ¡æœ‰æ•ˆè®°å½•ã€‚")
    return data

# è¿™æ˜¯åŸºäºä½ çš„é¢„å¤„ç†æ•°æ®æ–‡ä»¶çš„æ–°ç‰ˆæå–å‡½æ•°
def extract_features_from_preprocessed(data: pd.DataFrame, feature_order: List[str]) -> Optional[Tuple[pd.DataFrame, pd.Series]]:
    """ä»å·²ç»æ˜¯ç‰¹å¾æ ¼å¼çš„CSVä¸­æå–Xå’Œyã€‚"""
    print("--> æ­£åœ¨ä»é¢„å¤„ç†æ•°æ®ä¸­æå–ç‰¹å¾ (X) å’Œç›®æ ‡ (y)...")
    try:
        X = data[feature_order]
        y = data['weight']
        print("    æˆåŠŸæå–ç‰¹å¾å’Œç›®æ ‡ã€‚")
        return X, y
    except KeyError as e:
        print(f"    é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦çš„ç‰¹å¾åˆ—: {e}")
        return None, None

def apply_feature_transforms(X: pd.DataFrame, feature_order: List[str]) -> Tuple[pd.DataFrame, StandardScaler, RobustScaler]:
    """å¯¹ç‰¹å¾çŸ©é˜µåº”ç”¨ StandardScaler å’Œ RobustScalerã€‚"""
    print("--> æ­£åœ¨è¿›è¡Œç‰¹å¾å˜æ¢...")
    X_scaled = X.copy()
    
    std_scaler = StandardScaler()
    X_scaled[STD_SCALER_FEATURES] = std_scaler.fit_transform(X_scaled[STD_SCALER_FEATURES])
    print(f"    å·²åº”ç”¨ StandardScaler åˆ° {len(STD_SCALER_FEATURES)} ä¸ªç‰¹å¾ã€‚")

    robust_scaler = RobustScaler()
    X_scaled[ROBUST_SCALER_FEATURES] = robust_scaler.fit_transform(X_scaled[ROBUST_SCALER_FEATURES])
    print(f"    å·²åº”ç”¨ RobustScaler åˆ° {len(ROBUST_SCALER_FEATURES)} ä¸ªç‰¹å¾ã€‚")
    
    return X_scaled, std_scaler, robust_scaler

def train_model(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series) -> lgb.Booster:
    """è®­ç»ƒ LightGBM æ¨¡å‹ã€‚"""
    print("--> æ­£åœ¨è®­ç»ƒ LightGBM æ¨¡å‹...")
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    model = lgb.train(
        LGBM_PARAMS,
        train_data,
        valid_sets=[test_data],
        callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=True)]
    )
    return model

def save_model_and_config(model: lgb.Booster, std_scaler: StandardScaler, robust_scaler: RobustScaler, feature_order: List[str]):
    """ä¿å­˜æ¨¡å‹ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œå¹¶é™„åŠ å˜æ¢é…ç½®ã€‚"""
    print("--> æ­£åœ¨ä¿å­˜æ¨¡å‹åŠé…ç½®...")
    
    model.save_model(MODEL_FILE, num_iteration=model.best_iteration)
    print(f"    æ¨¡å‹ä¸»ä½“å·²ä¿å­˜ä¸ºæ–‡æœ¬æ ¼å¼åˆ°: {MODEL_FILE}")

    order_block = "[order]\n" + "".join([f"{i}={name}\n" for i, name in enumerate(feature_order)]) + "[/order]\n"
    
    std_indices = [feature_order.index(f) for f in STD_SCALER_FEATURES]
    robust_indices = [feature_order.index(f) for f in ROBUST_SCALER_FEATURES]
    
    definitions_block = "[definitions]\n"
    definitions_block += f"std_type=StandardScaler\nstd_features={','.join(map(str, std_indices))}\nstd_mean={','.join(map(str, std_scaler.mean_))}\nstd_scale={','.join(map(str, std_scaler.scale_))}\n\n"
    definitions_block += f"robust_type=RobustScaler\nrobust_features={','.join(map(str, robust_indices))}\nrobust_center={','.join(map(str, robust_scaler.center_))}\nrobust_scale={','.join(map(str, robust_scaler.scale_))}\n"
    definitions_block += "[/definitions]\n"
    
    transformed_indices = set(std_indices + robust_indices)
    untransformed_list = [f"{i}:{name}" for i, name in enumerate(feature_order) if i not in transformed_indices]

    final_transforms_block = (
        "\n\nend of trees\n\n"
        f"[transforms]\n{order_block}{definitions_block}untransformed_features={','.join(untransformed_list)}\ntransform=true\n[/transforms]\n"
    )
    
    with open(MODEL_FILE, 'a', encoding='utf-8') as f:
        f.write(final_transforms_block)
    print("    å˜æ¢é…ç½®å·²æˆåŠŸé™„åŠ åˆ°æ¨¡å‹æ–‡ä»¶æœ«å°¾ã€‚")

# ==============================================================================
# 3. ä¸»æ‰§è¡Œæµç¨‹ (MAIN EXECUTION)
# ==============================================================================

def main():
    """ä¸»å‡½æ•°ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰æ­¥éª¤ã€‚"""
    print("--- Mihomo æ¨¡å‹è®­ç»ƒå¼€å§‹ ---")
    
    try:
        parser = GoTransformParser(GO_FILE)
        feature_order = parser.get_feature_order()
    except Exception as e:
        print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        return
        
    full_data = load_and_clean_data(DATA_FILE)
    if full_data is None:
        return

    result = extract_features_from_preprocessed(full_data, feature_order)
    if result is None:
        return
    X, y = result

    X_scaled, std_scaler, robust_scaler = apply_feature_transforms(X, feature_order)

    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    model = train_model(X_train, y_train, X_test, y_test)
    
    save_model_and_config(model, std_scaler, robust_scaler, feature_order)
    
    print("\nğŸ‰ --- è®­ç»ƒå…¨éƒ¨å®Œæˆ --- ğŸ‰")
    print(f"æœ€ç»ˆæ¨¡å‹ '{MODEL_FILE}' å·²ç”Ÿæˆï¼Œéšæ—¶å¯ä»¥éƒ¨ç½²ï¼")

if __name__ == "__main__":
    main()